from __future__ import annotations

import asyncio
import hashlib
import json
import re
from datetime import datetime
from html import escape
from pathlib import Path
from typing import Any, Dict, List, Tuple

import aiohttp
import gradio as gr
import pandas as pd
from openpyxl.styles import Alignment, Border, Font, PatternFill, Side
from openpyxl.worksheet.page import PageMargins

from config_manager import LLMConfigManager
from extractor_core import VocabularySentenceExtractor
from llm_processor import (
    AsyncRunner,
    CheckpointManager,
    FatalAPIError,
    LLMClient,
    TransientAPIError,
)


SETTINGS_PATH = Path("llm_settings.json")
CHECKPOINT_PATH = Path("cache_results.jsonl")
TRACE_LOG_PATH = Path("run_trace.log")
BUILTIN_DICT_DIR = Path("dicts")
BUILTIN_IELTS_CSV = BUILTIN_DICT_DIR / "ielts_words.csv"
BUILTIN_BLACKLIST_CSV = BUILTIN_DICT_DIR / "common_blacklist.csv"
CONFIG_MANAGER = LLMConfigManager(SETTINGS_PATH)


def ensure_builtin_dict_assets() -> None:
    """åˆå§‹åŒ–å†…ç½®è¯åº“èµ„äº§ï¼Œé¿å…å®¹å™¨é¦–æ¬¡å¯åŠ¨ç¼ºæ–‡ä»¶ã€‚"""
    BUILTIN_DICT_DIR.mkdir(parents=True, exist_ok=True)
    if not BUILTIN_IELTS_CSV.exists():
        BUILTIN_IELTS_CSV.write_text(
            "word,level\n"
            "accommodate,5\n"
            "aberration,7\n"
            "mumble,6\n"
            "meticulous,7\n"
            "apple,1\n",
            encoding="utf-8",
        )
    if not BUILTIN_BLACKLIST_CSV.exists():
        BUILTIN_BLACKLIST_CSV.write_text(
            "word\nthe\nof\nand\nto\na\n",
            encoding="utf-8",
        )


ensure_builtin_dict_assets()


API_TYPE_TO_LABEL = {
    "openai_official": "OpenAIå®˜æ–¹",
    "openai_compatible": "å…¼å®¹OpenAI/ç¬¬ä¸‰æ–¹",
    "openai_responses": "å…¼å®¹OpenAI (ç‰¹æ®ŠResponsesåè®®)",
    "gemini_official": "Geminiå®˜æ–¹",
    "ollama_local": "Ollamaæœ¬åœ°",
}
LABEL_TO_API_TYPE = {v: k for k, v in API_TYPE_TO_LABEL.items()}


def now_text() -> str:
    return datetime.now().strftime("%H:%M:%S")


def calculate_file_hash(file_path: str, algorithm: str = "sha1") -> str:
    """æµå¼è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼Œé¿å…ä¸€æ¬¡æ€§è¯»å…¥å¤§æ–‡ä»¶ã€‚"""
    if algorithm.lower() != "sha1":
        raise ValueError("å½“å‰ä»…æ”¯æŒ sha1")

    hasher = hashlib.sha1()
    with open(file_path, "rb") as f:
        while True:
            chunk = f.read(8192)
            if not chunk:
                break
            hasher.update(chunk)
    return hasher.hexdigest()


def parse_provider_model(text: str) -> Tuple[str, str]:
    value = (text or "").strip()
    if ":" not in value:
        return "", ""
    provider, model = value.split(":", 1)
    return provider.strip(), model.strip()


def get_task_choices() -> Tuple[List[str], str]:
    pairs = CONFIG_MANAGER.get_provider_model_pairs()
    settings = CONFIG_MANAGER.load_settings()
    choices = [f"{provider}:{model}" for provider, model in pairs]

    default_provider = str(settings.get("default_provider", "")).strip()
    default_model = str(settings.get("default_model", "")).strip()
    default_choice = (
        f"{default_provider}:{default_model}"
        if default_provider and default_model
        else ""
    )

    if default_choice not in choices:
        default_choice = choices[0] if choices else ""
    return choices, default_choice


def get_provider_selector_choices() -> List[str]:
    return ["æ–°å¢æä¾›å•†", *CONFIG_MANAGER.get_provider_names()]


def map_extract_mode(label: str) -> str:
    text = (label or "").strip()
    if text == "ä»…è¯†åˆ«å•è¯":
        return "word_only"
    if text == "ä»…è¯†åˆ«å›ºå®šæ­é…":
        return "collocation_only"
    return "both"


def resolve_page_orientation(page_orientation: str, include_sentence: bool) -> str:
    choice = (page_orientation or "").strip()
    if choice == "ç«–å‘":
        return "portrait"
    if choice == "æ¨ªå‘":
        return "landscape"
    return "landscape" if include_sentence else "portrait"


def summarize_output_formats(selected_formats: List[str]) -> str:
    selected = selected_formats or []
    if not selected:
        return "**æœ¬æ¬¡å°†å¯¼å‡º**ï¼šæœªé€‰æ‹©æ ¼å¼ï¼ˆå°†å›é€€ä¸º Excelï¼‰"
    return f"**æœ¬æ¬¡å°†å¯¼å‡º**ï¼š{', '.join(selected)}"


def load_results_dataframe(
    cache_path: Path, word_sentence_map: Dict[str, str]
) -> pd.DataFrame:
    if not cache_path.exists():
        return pd.DataFrame(
            columns=[
                "word",
                "part_of_speech",
                "context_meaning",
                "collocation",
                "collocation_meaning",
                "sentence",
            ]
        )

    rows: List[Dict[str, Any]] = []
    with cache_path.open("r", encoding="utf-8") as f:
        for raw_line in f:
            line = raw_line.strip()
            if not line:
                continue
            try:
                item = json.loads(line)
            except json.JSONDecodeError:
                continue

            word = str(item.get("word", "")).strip()
            part_of_speech = str(item.get("part_of_speech", "")).strip()
            context_meaning = str(item.get("context_meaning", "")).strip()
            collocation = str(item.get("collocation", "")).strip()
            collocation_meaning = str(item.get("collocation_meaning", "")).strip()
            sentence = str(item.get("sentence", "")).strip() or word_sentence_map.get(
                word.lower(), ""
            )

            # ä¸è§£æå±‚ä¿æŒä¸€è‡´ï¼šæ­é…å ä½è¯ç»Ÿä¸€è§†ä¸ºç©ºå€¼ï¼Œä¿è¯â€œå•è¯ä¼˜å…ˆâ€ã€‚
            empty_markers = {
                "none",
                "null",
                "n/a",
                "no collocation",
                "na",
                "nil",
                "æ— ",
                "æ²¡æœ‰",
            }
            if collocation.lower() in empty_markers:
                collocation = ""
            if collocation_meaning.lower() in empty_markers:
                collocation_meaning = ""
            if not collocation:
                collocation_meaning = ""

            if word:
                rows.append(
                    {
                        "word": word,
                        "part_of_speech": part_of_speech,
                        "context_meaning": context_meaning,
                        "collocation": collocation,
                        "collocation_meaning": collocation_meaning,
                        "sentence": sentence,
                    }
                )

    return pd.DataFrame(rows)


def _generate_pdf(
    excel_df: pd.DataFrame,
    pdf_path: Path,
    include_sentence: bool,
    color_theme: str,
    page_orientation: str,
) -> None:
    """ä½¿ç”¨ ReportLab ç”Ÿæˆ PDFï¼ˆA4ï¼Œæ–¹å‘å¯é…ç½®ï¼Œä¸­æ–‡å¯è¯»ï¼Œå¼ºæ¸…æ´—ï¼‰ã€‚"""
    from reportlab.lib import colors
    from reportlab.lib.pagesizes import A4, landscape
    from reportlab.lib.styles import ParagraphStyle
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.cidfonts import UnicodeCIDFont
    from reportlab.platypus import Paragraph, SimpleDocTemplate, Table, TableStyle

    def _sanitize_pdf_text(value: Any) -> str:
        if value is None:
            return ""
        try:
            if pd.isna(value):
                return ""
        except Exception:
            pass
        text = str(value)
        # æ¸…ç†æ§åˆ¶å­—ç¬¦å¹¶è½¬ä¹‰ ReportLab æ®µè½æ ‡è®°ï¼Œé¿å…ç”Ÿæˆå¼‚å¸¸æˆ–æŸåå†…å®¹æµã€‚
        text = re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F]", "", text)
        text = text.strip().strip('"').strip("'")
        text = text.replace("\r\n", "\n").replace("\r", "\n").replace("\\n", "\n")
        text = escape(text, quote=False)
        return text.replace("\n", "<br/>")

    pdfmetrics.registerFont(UnicodeCIDFont("STSong-Light"))

    body_style = ParagraphStyle(
        "Body",
        fontName="STSong-Light",
        fontSize=10,
        leading=13,
        wordWrap="CJK",
    )
    header_style = ParagraphStyle(
        "Header",
        fontName="STSong-Light",
        fontSize=12,
        leading=14,
        alignment=1,  # center
        wordWrap="CJK",
    )

    headers = [str(c) for c in excel_df.columns]

    # å›ºå®šç»å¯¹åˆ—å®½ï¼ˆpointsï¼‰
    base_widths = {
        "å•è¯": 80,
        "è¯æ€§": 40,
        "è¯­å¢ƒé‡Šä¹‰": 100,
        "å›ºå®šæ­é…": 120,
        "æ­é…é‡Šä¹‰": 120,
        "åŸå¥": 340,
    }

    if include_sentence and "åŸå¥" in headers:
        col_widths = [base_widths.get(h, 100) for h in headers]
    else:
        # æ— åŸå¥ï¼šå°† 340 çš„å‰©ä½™å®½åº¦å¹³å‡åˆ†ç»™â€œé‡Šä¹‰/æ­é…â€åˆ—
        expandable = [h for h in headers if h in {"è¯­å¢ƒé‡Šä¹‰", "å›ºå®šæ­é…", "æ­é…é‡Šä¹‰"}]
        bonus = 340 // len(expandable) if expandable else 0
        col_widths = []
        for h in headers:
            width = base_widths.get(h, 100)
            if h in expandable:
                width += bonus
            if h == "åŸå¥":
                width = 0
            col_widths.append(width)

    pdf_orientation = resolve_page_orientation(page_orientation, include_sentence)
    pagesize = landscape(A4) if pdf_orientation == "landscape" else A4
    left_margin = 18
    right_margin = 18
    available_width = pagesize[0] - left_margin - right_margin
    total_width = sum(col_widths)
    if total_width > 0 and total_width > available_width:
        scale = available_width / total_width
        col_widths = [round(w * scale, 2) for w in col_widths]

    # æ„å»ºè¡¨æ ¼æ•°æ®
    data: List[List[Paragraph]] = []
    data.append(
        [Paragraph(f"<b>{_sanitize_pdf_text(h)}</b>", header_style) for h in headers]
    )
    for row in excel_df.itertuples(index=False):
        cleaned_row: List[Paragraph] = []
        for cell in row:
            cleaned_row.append(Paragraph(_sanitize_pdf_text(cell), body_style))
        data.append(cleaned_row)

    tmp_pdf_path = pdf_path.with_suffix(f"{pdf_path.suffix}.tmp")
    if tmp_pdf_path.exists():
        tmp_pdf_path.unlink()

    doc = SimpleDocTemplate(
        str(tmp_pdf_path),
        pagesize=pagesize,
        leftMargin=left_margin,
        rightMargin=right_margin,
        topMargin=18,
        bottomMargin=18,
    )

    table = Table(data, colWidths=col_widths, repeatRows=1)

    style_cmds = [
        ("FONTNAME", (0, 0), (-1, -1), "STSong-Light"),
        ("FONTSIZE", (0, 0), (-1, 0), 12),
        ("FONTSIZE", (0, 1), (-1, -1), 10),
        ("VALIGN", (0, 0), (-1, -1), "TOP"),
        ("GRID", (0, 0), (-1, -1), 0.5, colors.black),
        ("LEFTPADDING", (0, 0), (-1, -1), 4),
        ("RIGHTPADDING", (0, 0), (-1, -1), 4),
    ]

    # å¯¹é½ï¼šåŸå¥å·¦å¯¹é½ï¼Œå…¶ä½™å±…ä¸­
    style_cmds.append(("ALIGN", (0, 0), (-1, -1), "CENTER"))
    if "åŸå¥" in headers:
        sentence_idx = headers.index("åŸå¥")
        style_cmds.append(("ALIGN", (sentence_idx, 0), (sentence_idx, -1), "LEFT"))

    if color_theme == "æŠ¤çœ¼æ™¨é›¾ (ä½é¥±å’Œå½©)":
        style_cmds.append(("BACKGROUND", (0, 0), (-1, 0), colors.HexColor("#E7F3F6")))
        for row_idx in range(1, len(data)):
            if row_idx % 2 == 0:
                style_cmds.append(
                    (
                        "BACKGROUND",
                        (0, row_idx),
                        (-1, row_idx),
                        colors.HexColor("#F9F9F9"),
                    )
                )

    table.setStyle(TableStyle(style_cmds))
    try:
        doc.build([table])
        tmp_pdf_path.replace(pdf_path)
    finally:
        if tmp_pdf_path.exists():
            tmp_pdf_path.unlink()


def export_files(
    cache_path: Path,
    word_sentence_map: Dict[str, str],
    extract_mode: str,
    include_meaning: bool,
    include_sentence: bool,
    color_theme: str,
    page_orientation: str,
    output_formats: List[str],
) -> List[str]:
    def _sanitize_excel_text(value: Any) -> Any:
        if value is None:
            return ""
        text = str(value)
        # å»æ‰ Excel/OpenXML éæ³•æ§åˆ¶å­—ç¬¦ï¼Œé¿å… IllegalCharacterError
        return re.sub(r"[\x00-\x08\x0B\x0C\x0E-\x1F]", "", text)

    df = load_results_dataframe(cache_path, word_sentence_map)

    excel_cols = ["word", "part_of_speech"]
    if include_meaning:
        excel_cols.append("context_meaning")
    if extract_mode != "word_only":
        excel_cols.extend(["collocation", "collocation_meaning"])
    if include_sentence:
        excel_cols.append("sentence")

    for col in excel_cols:
        if col not in df.columns:
            df[col] = ""

    excel_df = df[excel_cols].rename(
        columns={
            "word": "å•è¯",
            "part_of_speech": "è¯æ€§",
            "context_meaning": "è¯­å¢ƒé‡Šä¹‰",
            "collocation": "å›ºå®šæ­é…",
            "collocation_meaning": "æ­é…é‡Šä¹‰",
            "sentence": "åŸå¥",
        }
    )
    excel_df = excel_df.map(_sanitize_excel_text)

    reading_excel = Path("vocabulary_reading.xlsx")
    typing_csv = Path("typing_world.csv")

    with pd.ExcelWriter(reading_excel, engine="openpyxl") as writer:
        excel_df.to_excel(writer, sheet_name="Sheet1", index=False)
        ws = writer.sheets["Sheet1"]

        # A4 æ‰“å°é€‚é…ï¼šæ”¯æŒè‡ªåŠ¨/ç«–å‘/æ¨ªå‘åˆ‡æ¢
        ws.page_setup.paperSize = 9  # A4
        ws.page_setup.orientation = resolve_page_orientation(
            page_orientation, include_sentence
        )
        ws.page_setup.fitToWidth = 1
        ws.page_setup.fitToHeight = 0
        ws.page_margins = PageMargins(left=0.75, right=0.75, top=0.75, bottom=0.75)
        ws.print_title_rows = "1:1"

        # åˆ—å®½ï¼šæŒ‰åˆ—è¯­ä¹‰æ˜ å°„ï¼Œé¿å…åŠ¨æ€åˆ—é”™ä½
        width_map = {
            "å•è¯": 18,
            "è¯æ€§": 10,
            "è¯­å¢ƒé‡Šä¹‰": 25,
            "å›ºå®šæ­é…": 25,
            "æ­é…é‡Šä¹‰": 25,
            "åŸå¥": 60,
        }
        header_by_col: Dict[int, str] = {}
        for col_idx in range(1, ws.max_column + 1):
            header = str(ws.cell(row=1, column=col_idx).value or "").strip()
            header_by_col[col_idx] = header
            col_letter = ws.cell(row=1, column=col_idx).column_letter
            ws.column_dimensions[col_letter].width = width_map.get(header, 20)

        # å­—ä½“/å¯¹é½/è¾¹æ¡†ï¼ˆé»‘ç™½é«˜å¯¹æ¯”ï¼‰
        header_font = Font(name="Arial", size=14, bold=True, color="000000")
        body_font = Font(name="Arial", size=12, bold=False, color="000000")
        align_center = Alignment(horizontal="center", vertical="center", wrap_text=True)
        align_sentence = Alignment(horizontal="left", vertical="center", wrap_text=True)
        border_color = "000000" if color_theme == "é»‘ç™½å…¬æ–‡ (æ‰“å°ä¸“ç”¨)" else "BFBFBF"
        thin_side = Side(style="thin", color=border_color)
        thin_border = Border(
            left=thin_side, right=thin_side, top=thin_side, bottom=thin_side
        )

        header_fill = PatternFill(fill_type=None)
        even_row_fill = PatternFill(fill_type=None)
        if color_theme == "æŠ¤çœ¼æ™¨é›¾ (ä½é¥±å’Œå½©)":
            header_fill = PatternFill(fill_type="solid", fgColor="E7F3F6")
            even_row_fill = PatternFill(fill_type="solid", fgColor="F9F9F9")

        for row_idx in range(1, ws.max_row + 1):
            for col_idx in range(1, ws.max_column + 1):
                cell = ws.cell(row=row_idx, column=col_idx)
                if header_by_col.get(col_idx) == "åŸå¥":
                    cell.alignment = align_sentence
                else:
                    cell.alignment = align_center
                cell.border = thin_border
                if row_idx == 1:
                    cell.font = header_font
                    cell.fill = header_fill
                else:
                    cell.font = body_font
                    if color_theme == "æŠ¤çœ¼æ™¨é›¾ (ä½é¥±å’Œå½©)" and row_idx % 2 == 0:
                        cell.fill = even_row_fill

    typing_source_col = "context_meaning"
    if typing_source_col not in df.columns:
        df[typing_source_col] = ""
    typing_df = df[["word", typing_source_col]].fillna("")
    typing_df = typing_df.map(_sanitize_excel_text)
    typing_df.to_csv(typing_csv, index=False, header=False, sep=",")

    output_files: List[str] = []
    if "Excel (.xlsx)" in output_formats:
        output_files.append(str(reading_excel.resolve()))

    if "PDF (.pdf)" in output_formats:
        pdf_path = Path("vocabulary_reading.pdf")
        try:
            _generate_pdf(
                excel_df,
                pdf_path,
                include_sentence=include_sentence,
                color_theme=color_theme,
                page_orientation=page_orientation,
            )
            output_files.append(str(pdf_path.resolve()))
        except Exception as exc:
            print(f"[Warning] PDF ç”Ÿæˆå¤±è´¥ï¼Œå·²è·³è¿‡ï¼Œä»…ä¿ç•™ Excel: {exc}")
            if pdf_path.exists():
                try:
                    pdf_path.unlink()
                except OSError:
                    pass
            if str(reading_excel.resolve()) not in output_files:
                output_files.append(str(reading_excel.resolve()))

    if "TXT å•è¯æœ¬ (é€‚é…å¢¨å¢¨/æ¬§è·¯)" in output_formats:
        txt_path = Path("maimemo_vocabulary.txt")
        words = (
            df["word"]
            .fillna("")
            .astype(str)
            .str.strip()
            .str.lower()
            .drop_duplicates()
            .tolist()
        )
        words = [w for w in words if w]
        txt_path.write_text("\n".join(words), encoding="utf-8")
        output_files.append(str(txt_path.resolve()))

    if not output_files:
        output_files.append(str(reading_excel.resolve()))

    return output_files


async def process_pipeline(
    provider_model: str,
    min_level: float,
    extract_mode_label: str,
    include_meaning: bool,
    include_sentence: bool,
    color_theme: str,
    page_orientation: str,
    output_formats: List[str],
    filter_strategy_label: str,
    pdf_file: str,
    custom_csv_file: str,
):
    logs: List[str] = []

    def push(msg: str) -> str:
        line = f"[{now_text()}] {msg}"
        logs.append(line)
        with TRACE_LOG_PATH.open("a", encoding="utf-8") as f:
            f.write(line + "\n")
        return "\n".join(logs)

    try:
        if not pdf_file:
            yield push("è¯·å…ˆä¸Šä¼ ç”µå­ä¹¦æ–‡ä»¶ã€‚"), []
            return

        provider_name, model_name = parse_provider_model(provider_model)
        if not provider_name or not model_name:
            yield push("è¯·é€‰æ‹©æœ‰æ•ˆçš„â€œæä¾›å•†:æ¨¡å‹â€é…ç½®ã€‚"), []
            return

        filename = Path(pdf_file).name
        file_hash = calculate_file_hash(pdf_file, algorithm="sha1")
        book_key = f"{file_hash[:8]}_{filename}"
        yield push(f"æ–‡ä»¶æŒ‡çº¹: {book_key}"), []

        provider = CONFIG_MANAGER.get_provider(provider_name)
        if not provider:
            yield (
                push(f"é…ç½®ä¸å­˜åœ¨ï¼š{provider_name}ã€‚è¯·åœ¨â€œæ¨¡å‹é…ç½®ä¸­å¿ƒâ€ä¿å­˜åé‡è¯•ã€‚"),
                [],
            )
            return

        api_type = (
            str(provider.get("api_type", "openai_compatible")).strip()
            or "openai_compatible"
        )
        base_url = str(provider.get("base_url", "")).strip()
        api_key = str(provider.get("api_key", "")).strip()
        if not base_url:
            yield push(f"æä¾›å•† {provider_name} ç¼ºå°‘ Base URLï¼Œè¯·åˆ°é…ç½®ä¸­å¿ƒè¡¥å…¨ã€‚"), []
            return

        extract_mode = map_extract_mode(extract_mode_label)
        yield (
            push(
                f"å¼€å§‹ä»»åŠ¡ï¼Œä½¿ç”¨é…ç½®={provider_name}:{model_name}ï¼Œæœ€ä½ç­‰çº§={int(min_level)}ï¼Œæå–æ¨¡å¼={extract_mode_label}ï¼Œé¡µé¢æ–¹å‘={page_orientation}"
            ),
            [],
        )

        csv_path: str | None = None
        filter_strategy = "whitelist"
        effective_min_level = float(min_level)
        if custom_csv_file:
            csv_path = custom_csv_file
            yield push("æ£€æµ‹åˆ°è‡ªå®šä¹‰è¯åº“ï¼šå°†è¦†ç›–å†…ç½®è¯åº“é…ç½®ã€‚"), []

            try:
                custom_df = pd.read_csv(csv_path, encoding="utf-8-sig")
                has_level = "level" in custom_df.columns
            except Exception:
                has_level = False

            file_name_lower = Path(csv_path).name.lower()
            if has_level:
                filter_strategy = "whitelist"
                yield push("æ£€æµ‹åˆ°è‡ªå®šä¹‰è¯åº“åŒ…å« level åˆ—ï¼Œè‡ªåŠ¨ä½¿ç”¨ç™½åå•ç­–ç•¥ã€‚"), []
            elif "black" in file_name_lower:
                filter_strategy = "blacklist"
                effective_min_level = 0
                yield push("æ£€æµ‹åˆ°è‡ªå®šä¹‰é»‘åå•è¯åº“æ–‡ä»¶åï¼Œè‡ªåŠ¨ä½¿ç”¨é»‘åå•ç­–ç•¥ã€‚"), []
            else:
                filter_strategy = "whitelist"
                yield push("æœªè¯†åˆ«åˆ°é»‘åå•ç‰¹å¾ï¼Œé»˜è®¤æŒ‰ç™½åå•è¯åº“å¤„ç†ã€‚"), []
        else:
            if filter_strategy_label == "é›…æ€å¤‡è€ƒæ¨¡å¼ (ç™½åå•)":
                if BUILTIN_IELTS_CSV.exists():
                    csv_path = str(BUILTIN_IELTS_CSV)
                    yield push(f"ä½¿ç”¨å†…ç½®è¯åº“ï¼š{BUILTIN_IELTS_CSV}"), []
                else:
                    yield push(f"ä»»åŠ¡å¤±è´¥: æœªæ‰¾åˆ°å†…ç½®è¯åº“æ–‡ä»¶ {BUILTIN_IELTS_CSV}"), []
                    return
            else:
                if BUILTIN_BLACKLIST_CSV.exists():
                    csv_path = str(BUILTIN_BLACKLIST_CSV)
                    filter_strategy = "blacklist"
                    effective_min_level = 0
                    yield push(f"ä½¿ç”¨å†…ç½®é»‘åå•ï¼š{BUILTIN_BLACKLIST_CSV}"), []
                else:
                    yield (
                        push(f"ä»»åŠ¡å¤±è´¥: æœªæ‰¾åˆ°å†…ç½®é»‘åå•æ–‡ä»¶ {BUILTIN_BLACKLIST_CSV}"),
                        [],
                    )
                    return

        extractor = VocabularySentenceExtractor()
        if filter_strategy == "whitelist":
            yield push("æ­£åœ¨æŒ‰ç™½åå•è¯åº“ä¸ç­‰çº§æ¡ä»¶è¿›è¡Œè¿‡æ»¤..."), []
            extractor.load_vocabulary_from_csv(csv_path, min_level=effective_min_level)
            progress_stream = extractor.extract_with_progress(
                pdf_file,
                filter_strategy="whitelist",
                max_extracted_words=0,
                progress_every_sentences=100,
            )
        else:
            yield push("æ­£åœ¨æŒ‰é»‘åå•ç­–ç•¥æå–ä¸“ä¸šè¯...ï¼ˆå·²å¿½ç•¥æœ€ä½ç­‰çº§ï¼‰"), []
            extractor.load_blacklist_from_csv(csv_path)
            progress_stream = extractor.extract_with_progress(
                pdf_file,
                filter_strategy="blacklist",
                max_extracted_words=0,
                progress_every_sentences=100,
            )

        word_sentence_map: Dict[str, str] = {}
        parse_done_event: Dict[str, Any] = {}
        for event in progress_stream:
            status = str(event.get("status", "")).strip()
            if status == "parsing":
                message = str(event.get("message", "")).strip()
                if message:
                    yield push(message), []
                continue
            if status == "done":
                parse_done_event = event
                data = event.get("data", {})
                if isinstance(data, dict):
                    word_sentence_map = data
                break

        if parse_done_event.get("truncated"):
            limit = parse_done_event.get("limit", 0)
            yield push(f"è§£æå·²è§¦å‘ä¸Šé™ä¿æŠ¤ï¼šæœ€å¤šæå– {limit} ä¸ªå€™é€‰è¯ã€‚"), []

        yield push(f"ç”µå­ä¹¦è§£æå®Œæˆï¼Œå‘½ä¸­å»é‡è¯æ±‡æ•°={len(word_sentence_map)}"), []

        checkpoint = CheckpointManager(CHECKPOINT_PATH)
        processed = checkpoint.load_processed_words()
        pending_items = checkpoint.filter_pending(
            word_sentence_map, book_key=book_key, extract_mode=extract_mode
        )
        yield (
            push(f"æ–­ç‚¹æ¢å¤å®Œæˆï¼Œå·²å¤„ç†={len(processed)}ï¼Œå¾…å¤„ç†={len(pending_items)}"),
            [],
        )

        client = LLMClient(
            api_type=api_type,
            base_url=base_url,
            api_key=api_key,
            model_name=model_name,
        )

        yield push("æ­£åœ¨è¿›è¡Œé¢„æ£€ï¼ˆPreflightï¼‰..."), []
        try:
            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30)
            ) as preflight_session:
                await client.preflight_check(preflight_session)
        except FatalAPIError as exc:
            yield (
                push(
                    f"ã€ä¸¥é‡é”™è¯¯ï¼šé¢„æ£€å¤±è´¥ï¼Œä»»åŠ¡ç»ˆæ­¢ï¼è¯·æ£€æŸ¥æ¨¡å‹é…ç½®ã€API Key æˆ–æ¥å£åè®®æ˜¯å¦æ­£ç¡®ã€‘{exc}"
                ),
                [],
            )
            return
        except TransientAPIError as exc:
            yield push(f"ã€é¢„æ£€å¤±è´¥ï¼šç½‘ç»œæˆ–æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‘{exc}"), []
            return

        runner = AsyncRunner(client, checkpoint, max_concurrency=10)
        progress_queue: asyncio.Queue[str] = asyncio.Queue()

        yield push("å¼€å§‹è¯·æ±‚ LLMï¼Œå¹¶å‘æ± å¯åŠ¨ï¼ˆSemaphore=10ï¼‰..."), []
        run_task = asyncio.create_task(
            runner.run(
                pending_items,
                progress_queue=progress_queue,
                book_key=book_key,
                extract_mode=extract_mode,
            )
        )

        while True:
            if run_task.done() and progress_queue.empty():
                break
            try:
                progress_line = await asyncio.wait_for(
                    progress_queue.get(), timeout=0.3
                )
                yield push(progress_line), []
            except asyncio.TimeoutError:
                if run_task.done():
                    break

        results = await run_task
        yield push(f"LLM é˜¶æ®µå®Œæˆï¼ŒæˆåŠŸå¤„ç†={len(results)}"), []

        yield push("æ­£åœ¨å¯¼å‡º Excel å’Œ Typing-World CSV..."), []
        output_paths = export_files(
            CHECKPOINT_PATH,
            word_sentence_map,
            extract_mode,
            include_meaning=include_meaning,
            include_sentence=include_sentence,
            color_theme=color_theme,
            page_orientation=page_orientation,
            output_formats=output_formats,
        )
        yield push("å¯¼å‡ºå®Œæˆï¼Œå¯ä¸‹è½½ç»“æœæ–‡ä»¶ã€‚"), output_paths

    except asyncio.CancelledError:
        yield push("ä»»åŠ¡å·²åœæ­¢ã€‚"), []
        raise
    except FatalAPIError as exc:
        yield push(f"ã€ä¸¥é‡é”™è¯¯ã€‘{exc}ï¼Œä»»åŠ¡å·²ç»ˆæ­¢ã€‚"), []
    except TransientAPIError as exc:
        yield push(f"ã€ä¸´æ—¶é”™è¯¯ã€‘{exc}ï¼Œè¯·ç¨åé‡è¯•ã€‚"), []
    except RuntimeError as exc:
        if "è¿ç»­å¤±è´¥è¶…è¿‡é˜ˆå€¼" in str(exc):
            yield push(f"ã€ç†”æ–­è§¦å‘ã€‘{exc}"), []
            return
        yield push(f"ä»»åŠ¡å¤±è´¥: {exc}"), []
    except Exception as exc:
        yield push(f"ä»»åŠ¡å¤±è´¥: {exc}"), []


def on_provider_for_edit_change(provider_for_edit: str):
    if provider_for_edit == "æ–°å¢æä¾›å•†":
        return "", "å…¼å®¹OpenAI/ç¬¬ä¸‰æ–¹", "", "", gr.update(choices=[], value=None), ""

    provider = CONFIG_MANAGER.get_provider(provider_for_edit)
    if not provider:
        return (
            "",
            "å…¼å®¹OpenAI/ç¬¬ä¸‰æ–¹",
            "",
            "",
            gr.update(choices=[], value=None),
            "æœªæ‰¾åˆ°è¯¥æä¾›å•†é…ç½®",
        )

    api_type = str(provider.get("api_type", "openai_compatible"))
    api_label = API_TYPE_TO_LABEL.get(api_type, "å…¼å®¹OpenAI/ç¬¬ä¸‰æ–¹")
    base_url = str(provider.get("base_url", ""))
    api_key = str(provider.get("api_key", ""))
    models = [str(m).strip() for m in provider.get("models", []) if str(m).strip()]
    model_value = models[0] if models else None

    return (
        provider_for_edit,
        api_label,
        base_url,
        api_key,
        gr.update(choices=models, value=model_value),
        "",
    )


async def on_fetch_models(api_type_label: str, base_url: str, api_key: str):
    api_type = LABEL_TO_API_TYPE.get(api_type_label, "openai_compatible")
    models, err = await CONFIG_MANAGER.fetch_models(api_type, base_url, api_key)
    if err:
        gr.Warning(f"æ‹‰å–æ¨¡å‹å¤±è´¥ï¼š{err}ã€‚ä½ å¯ä»¥æ‰‹åŠ¨è¾“å…¥æ¨¡å‹åã€‚")
        return gr.update(choices=[], value=None), f"æ‹‰å–å¤±è´¥ï¼š{err}"

    if not models:
        gr.Warning("æœªæ¢æµ‹åˆ°æ¨¡å‹åˆ—è¡¨ã€‚è¯·æ‰‹åŠ¨è¾“å…¥æ¨¡å‹ååä¿å­˜ã€‚")
        return gr.update(choices=[], value=None), "æœªæ¢æµ‹åˆ°æ¨¡å‹ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥ã€‚"

    return gr.update(
        choices=models, value=models[0]
    ), f"æ‹‰å–æˆåŠŸï¼Œå…± {len(models)} ä¸ªæ¨¡å‹"


def on_save_provider(
    provider_for_edit: str,
    provider_name: str,
    api_type_label: str,
    base_url: str,
    api_key: str,
    selected_model: str,
):
    name = (provider_name or "").strip()
    model_name = (selected_model or "").strip()
    if not name:
        return (
            gr.update(),
            gr.update(),
            gr.update(),
            "ä¿å­˜å¤±è´¥ï¼šæä¾›å•†åç§°ä¸èƒ½ä¸ºç©º",
        )
    if not model_name:
        return (
            gr.update(),
            gr.update(),
            gr.update(),
            "ä¿å­˜å¤±è´¥ï¼šè¯·å…ˆé€‰æ‹©æˆ–è¾“å…¥æ¨¡å‹åç§°",
        )

    api_type = LABEL_TO_API_TYPE.get(api_type_label, "openai_compatible")
    existing = CONFIG_MANAGER.get_provider(name)
    models: List[str] = []
    if existing and isinstance(existing.get("models"), list):
        models.extend(
            [str(m).strip() for m in existing.get("models", []) if str(m).strip()]
        )
    if model_name not in models:
        models.append(model_name)

    if provider_for_edit != "æ–°å¢æä¾›å•†" and provider_for_edit != name:
        CONFIG_MANAGER.delete_provider(provider_for_edit)

    CONFIG_MANAGER.upsert_provider(
        provider_name=name,
        api_type=api_type,
        base_url=(base_url or "").strip(),
        api_key=(api_key or "").strip(),
        models=models,
        default_model=model_name,
    )

    task_choices, task_default = get_task_choices()
    provider_choices = get_provider_selector_choices()
    return (
        gr.update(choices=task_choices, value=task_default),
        gr.update(choices=provider_choices, value=name),
        gr.update(choices=models, value=model_name),
        f"ä¿å­˜æˆåŠŸï¼š{name}:{model_name}",
    )


with gr.Blocks(title="Vocabulary Pipeline") as demo:
    with gr.Tabs():
        with gr.Tab("ä»»åŠ¡çœ‹æ¿"):
            with gr.Row():
                with gr.Column(scale=1):
                    choices, default_choice = get_task_choices()
                    provider_model_dropdown = gr.Dropdown(
                        label="æ¨¡å‹é…ç½®ï¼ˆæä¾›å•†:æ¨¡å‹ï¼‰",
                        choices=choices,
                        value=default_choice if default_choice else None,
                        allow_custom_value=False,
                    )
                    pdf_file = gr.File(
                        label="ä¸Šä¼ ç”µå­ä¹¦",
                        file_types=[".pdf", ".epub", ".txt"],
                        type="filepath",
                    )
                    min_level = gr.Slider(
                        label="æœ€ä½è¯æ±‡ç­‰çº§ (å¦‚é›…æ€åˆ†æ•°)",
                        minimum=0,
                        maximum=9,
                        step=1,
                        value=0,
                    )
                    extract_mode_label = gr.Radio(
                        label="æå–æ¨¡å¼",
                        choices=["å…¨éƒ¨è¯†åˆ«", "ä»…è¯†åˆ«å•è¯", "ä»…è¯†åˆ«å›ºå®šæ­é…"],
                        value="å…¨éƒ¨è¯†åˆ«",
                    )
                    with gr.Row():
                        include_meaning = gr.Checkbox(
                            label="ğŸ“œ å¯¼å‡ºè¯­å¢ƒé‡Šä¹‰", value=True
                        )
                        include_sentence = gr.Checkbox(
                            label="ğŸ“– å¯¼å‡ºåŸæ–‡å¥å­", value=True
                        )
                    color_theme = gr.Radio(
                        ["é»‘ç™½å…¬æ–‡ (æ‰“å°ä¸“ç”¨)", "æŠ¤çœ¼æ™¨é›¾ (ä½é¥±å’Œå½©)"],
                        label="ğŸ¨ æ’ç‰ˆä¸»é¢˜",
                        value="é»‘ç™½å…¬æ–‡ (æ‰“å°ä¸“ç”¨)",
                    )
                    page_orientation = gr.Radio(
                        ["è‡ªåŠ¨ (æœ‰åŸå¥åˆ™æ¨ªå‘)", "ç«–å‘", "æ¨ªå‘"],
                        label="ğŸ§­ é¡µé¢æ–¹å‘",
                        value="è‡ªåŠ¨ (æœ‰åŸå¥åˆ™æ¨ªå‘)",
                    )
                    output_formats = gr.CheckboxGroup(
                        ["Excel (.xlsx)", "PDF (.pdf)", "TXT å•è¯æœ¬ (é€‚é…å¢¨å¢¨/æ¬§è·¯)"],
                        label="ğŸ’¾ å¯¼å‡ºæ ¼å¼",
                        value=["Excel (.xlsx)"],
                    )
                    export_preview = gr.Markdown(
                        summarize_output_formats(["Excel (.xlsx)"])
                    )
                    filter_strategy_choice = gr.Radio(
                        label="è¿‡æ»¤ç­–ç•¥",
                        choices=["ä¸“ä¸šé˜…è¯»æ¨¡å¼ (æ¨è)", "é›…æ€å¤‡è€ƒæ¨¡å¼ (ç™½åå•)"],
                        value="ä¸“ä¸šé˜…è¯»æ¨¡å¼ (æ¨è)",
                    )
                    gr.Markdown(
                        "**è‡ªå®šä¹‰è¯åº“ï¼ˆå¯é€‰ï¼‰è¯´æ˜**ï¼šä¸Šä¼ åå°†è¦†ç›–å†…ç½®ç­–ç•¥ã€‚"
                        "è‹¥ä¸º `word,level` ä¸¤åˆ—é»˜è®¤æŒ‰ç™½åå•å¤„ç†ï¼›"
                        "è‹¥æ–‡ä»¶ååŒ…å« black å°†è‡ªåŠ¨æŒ‰é»‘åå•å¤„ç†ã€‚"
                    )
                    csv_file = gr.File(
                        label="è‡ªå®šä¹‰è¯åº“ (å¯é€‰ï¼Œå°†è¦†ç›–å†…ç½®è¯åº“)", type="filepath"
                    )
                    start_button = gr.Button("å¯åŠ¨ä»»åŠ¡", variant="primary")
                    stop_button = gr.Button("åœæ­¢ä»»åŠ¡", variant="stop")

                    output_formats.change(
                        fn=summarize_output_formats,
                        inputs=[output_formats],
                        outputs=[export_preview],
                    )

                with gr.Column(scale=1):
                    log_box = gr.Textbox(label="è¿è¡Œæ—¥å¿—", lines=28, interactive=False)
                    output_files = gr.File(
                        label="ä¸‹è½½æ–‡ä»¶", file_count="multiple", interactive=False
                    )

            run_event = start_button.click(
                fn=process_pipeline,
                inputs=[
                    provider_model_dropdown,
                    min_level,
                    extract_mode_label,
                    include_meaning,
                    include_sentence,
                    color_theme,
                    page_orientation,
                    output_formats,
                    filter_strategy_choice,
                    pdf_file,
                    csv_file,
                ],
                outputs=[log_box, output_files],
            )
            stop_button.click(fn=None, cancels=[run_event])

        with gr.Tab("æ¨¡å‹é…ç½®ä¸­å¿ƒ"):
            provider_for_edit = gr.Dropdown(
                label="é€‰æ‹©æä¾›å•†",
                choices=get_provider_selector_choices(),
                value="æ–°å¢æä¾›å•†",
            )
            provider_name = gr.Textbox(
                label="æä¾›å•†åç§°", placeholder="ä¾‹å¦‚: my-provider"
            )
            api_type = gr.Dropdown(
                label="æ¥å£ç±»å‹",
                choices=list(LABEL_TO_API_TYPE.keys()),
                value="å…¼å®¹OpenAI/ç¬¬ä¸‰æ–¹",
            )
            base_url = gr.Textbox(
                label="Base URL", placeholder="ä¾‹å¦‚: https://api.openai.com/v1"
            )
            api_key = gr.Textbox(label="API Key", type="password", value="")
            fetch_models_button = gr.Button("ğŸ”„ æµ‹è¯•å¹¶æ‹‰å–æ¨¡å‹")
            models_dropdown = gr.Dropdown(
                label="æ¨¡å‹åˆ—è¡¨ï¼ˆå¯æ‰‹åŠ¨è¾“å…¥ï¼‰",
                choices=[],
                value=None,
                allow_custom_value=True,
            )
            save_button = gr.Button("ğŸ’¾ ä¿å­˜æ­¤é…ç½®", variant="primary")
            config_status = gr.Textbox(label="é…ç½®çŠ¶æ€", interactive=False)

            provider_for_edit.change(
                fn=on_provider_for_edit_change,
                inputs=[provider_for_edit],
                outputs=[
                    provider_name,
                    api_type,
                    base_url,
                    api_key,
                    models_dropdown,
                    config_status,
                ],
            )

            fetch_models_button.click(
                fn=on_fetch_models,
                inputs=[api_type, base_url, api_key],
                outputs=[models_dropdown, config_status],
            )

            save_button.click(
                fn=on_save_provider,
                inputs=[
                    provider_for_edit,
                    provider_name,
                    api_type,
                    base_url,
                    api_key,
                    models_dropdown,
                ],
                outputs=[
                    provider_model_dropdown,
                    provider_for_edit,
                    models_dropdown,
                    config_status,
                ],
            )


if __name__ == "__main__":
    demo.queue().launch(server_name="0.0.0.0", server_port=7860)
